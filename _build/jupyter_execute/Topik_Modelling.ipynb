{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79C8eWKB1P4M"
   },
   "source": [
    "# Topik Modelling Dengan Latent Semantic Indexing (LSI) atau Latent Semantic Analysis (LSA) menggunakan Scikit-Learn\n",
    "Dalam pembahasan kali ini, kita akan fokus pada Latent Semantic Indexing (LSI) atau Latent Semantic Analysis (LSA) dan melakukan topik modelling menggunakan Scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxJVH7Ha23vO"
   },
   "source": [
    "## **Topik Modelling**\n",
    "Topik Modelling ialah teknik tanpa pengawasan untuk menemukan tema dokumen yang diberikan. Ini mengekstrak kumpulan kata kunci yang terjadi bersama. Kata kunci yang muncul bersama ini mewakili sebuah topik. Misalnya, saham, pasar, ekuitas, reksa dana akan mewakili topik 'investasi saham'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysFKklwn3F_H"
   },
   "source": [
    "## **Latent Semantic Indexing (LSI) atau Latent Semantic Analysis (LSA)**\n",
    "Latent Semantic Indexing (LSI) atau Latent Semantic Analysis (LSA)  adalah teknik dalam natural language processing , khususnya  distributional semantics , yang menganalisis hubungan antara satu set dokumen dan istilah yang dikandungnya dengan menghasilkan satu set konsep yang terkait dengan dokumen dan istilah. LSA mengasumsikan bahwa kata-kata yang memiliki makna yang dekat akan muncul dalam potongan teks yang serupa (  distributional hypothesis ). Sebuah matriks yang berisi jumlah kata per dokumen (baris mewakili kata-kata unik dan kolom mewakili setiap dokumen) dibangun dari sepotong besar teks dan teknik matematika yang disebut Singular Value Decomposition (SVD) digunakan untuk mengurangi jumlah baris dengan tetap menjaga kesamaan struktur antar kolom. Dokumen kemudian dibandingkan dengan mengambil kosinus sudut antara dua vektor (atau produk titik antara normalisasi dua vektor) yang dibentuk oleh dua kolom. Nilai yang mendekati 1 menunjukkan dokumen yang sangat mirip sedangkan nilai yang mendekati 0 menunjukkan dokumen yang sangat berbeda.<br>\n",
    "<center><img src='https://media.geeksforgeeks.org/wp-content/uploads/20210406165951/Screenshot20210406165933.png'></center><center>Gambar LSA</center> Untuk melakukan LSA dapat dilakukan dengan mengikuti tahapan tahapan berikut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXtSvKfs3oiP"
   },
   "source": [
    "## **Mengambil Dokumen**\n",
    "Langkah awal untuk melakukan Topik Modelling ialah dengan mengambil dokumen tersebut dengan mengcrawling data dokumen dengan menggunakan scrapy & crochet seperti berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24201,
     "status": "ok",
     "timestamp": 1666651046771,
     "user": {
      "displayName": "sumina so",
      "userId": "07390640577398086821"
     },
     "user_tz": -420
    },
    "id": "kBDcPqq_nXi_",
    "outputId": "3640c6b3-7c95-4c7d-94c5-b0b475da7c8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: scrapy in /usr/local/lib/python3.8/dist-packages (2.7.1)\n",
      "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.8/dist-packages (from scrapy) (38.0.4)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from scrapy) (2.1.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from scrapy) (1.7.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from scrapy) (21.3)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in /usr/local/lib/python3.8/dist-packages (from scrapy) (5.5.2)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.8/dist-packages (from scrapy) (2.0.6)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from scrapy) (57.4.0)\n",
      "Requirement already satisfied: lxml>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from scrapy) (4.9.1)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in /usr/local/lib/python3.8/dist-packages (from scrapy) (22.10.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from scrapy) (1.0.6)\n",
      "Requirement already satisfied: tldextract in /usr/local/lib/python3.8/dist-packages (from scrapy) (3.4.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.8/dist-packages (from scrapy) (0.2.1)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.8/dist-packages (from scrapy) (22.1.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from scrapy) (0.7.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.8/dist-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in /usr/local/lib/python3.8/dist-packages (from scrapy) (21.1.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.8/dist-packages (from cryptography>=3.3->scrapy) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.12->cryptography>=3.3->scrapy) (2.21)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.8/dist-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from protego>=0.1.15->scrapy) (1.15.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.8/dist-packages (from service-identity>=18.1.0->scrapy) (21.4.0)\n",
      "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.8/dist-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.8/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.8/dist-packages (from Twisted>=18.9.0->scrapy) (4.1.1)\n",
      "Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.8/dist-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.8/dist-packages (from Twisted>=18.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.8/dist-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.8/dist-packages (from hyperlink>=17.1.1->Twisted>=18.9.0->scrapy) (2.10)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->scrapy) (3.0.9)\n",
      "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from tldextract->scrapy) (2.28.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.8/dist-packages (from tldextract->scrapy) (3.8.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.8/dist-packages (from tldextract->scrapy) (1.5.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.1.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: crochet in /usr/local/lib/python3.8/dist-packages (2.0.0)\n",
      "Requirement already satisfied: Twisted>=16.0 in /usr/local/lib/python3.8/dist-packages (from crochet) (22.10.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.8/dist-packages (from crochet) (1.14.1)\n",
      "Requirement already satisfied: zope.interface>=4.4.2 in /usr/local/lib/python3.8/dist-packages (from Twisted>=16.0->crochet) (5.5.2)\n",
      "Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from Twisted>=16.0->crochet) (22.10.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from Twisted>=16.0->crochet) (21.4.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.8/dist-packages (from Twisted>=16.0->crochet) (21.0.0)\n",
      "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.8/dist-packages (from Twisted>=16.0->crochet) (15.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.8/dist-packages (from Twisted>=16.0->crochet) (4.1.1)\n",
      "Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.8/dist-packages (from Twisted>=16.0->crochet) (22.10.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from Automat>=0.8.0->Twisted>=16.0->crochet) (1.15.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.8/dist-packages (from hyperlink>=17.1.1->Twisted>=16.0->crochet) (2.10)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from zope.interface>=4.4.2->Twisted>=16.0->crochet) (57.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy\n",
    "!pip install crochet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MjU0dNBRndL-"
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "import re\n",
    "from crochet import setup, wait_for\n",
    "setup()\n",
    "\n",
    "class QuotesToCsv(scrapy.Spider):\n",
    "    name = \"MJKQuotesToCsv\"\n",
    "    start_urls = [\n",
    "        'https://tekno.tempo.co/read/1580340/peran-penting-iptekin-terhadap-kemajuan-sebuah-bangsa'\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        'ITEM_PIPELINES': {\n",
    "            '__main__.ExtractFirstLine': 1\n",
    "        },\n",
    "        'FEEDS': {\n",
    "            'news.csv': {\n",
    "                'format': 'csv',\n",
    "                'overwrite': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"parse data from urls\"\"\"\n",
    "        for quote in response.css('#isi > p'):\n",
    "            yield {'news': quote.extract()}\n",
    "\n",
    "\n",
    "class ExtractFirstLine(object):\n",
    "    def process_item(self, item, spider):\n",
    "        \"\"\"text processing\"\"\"\n",
    "        lines = dict(item)[\"news\"].splitlines()\n",
    "        first_line = self.__remove_html_tags__(lines[0])\n",
    "\n",
    "        return {'news': first_line}\n",
    "\n",
    "    def __remove_html_tags__(self, text):\n",
    "        \"\"\"remove html tags from string\"\"\"\n",
    "        html_tags = re.compile('<.*?>')\n",
    "        return re.sub(html_tags, '', text)\n",
    "\n",
    "@wait_for(10)\n",
    "def run_spider():\n",
    "    \"\"\"run spider with MJKQuotesToCsv\"\"\"\n",
    "    crawler = CrawlerRunner()\n",
    "    d = crawler.crawl(QuotesToCsv)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1715,
     "status": "ok",
     "timestamp": 1666651049067,
     "user": {
      "displayName": "sumina so",
      "userId": "07390640577398086821"
     },
     "user_tz": -420
    },
    "id": "hLvP-t2ISp3c",
    "outputId": "bb6df657-a94d-432a-9199-811eb775aaad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:twisted:/usr/local/lib/python3.8/dist-packages/scrapy/utils/request.py:231: scrapy.exceptions.ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n"
     ]
    }
   ],
   "source": [
    "run_spider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jnEnUou4Nnc"
   },
   "source": [
    "## **Meload Dokumen**\n",
    "Setelah tahapan mengambil dokumen selesai, selanjutnya meload dokumen yang sudah didapatkan. Untuk dapat meload dokumen kita gunakan library os dan pandas seperti berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4McryBFvteQn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load Dataset\n",
    "documents_list = []\n",
    "with open( os.path.join(\"news.csv\") ,\"r\") as fin:\n",
    "    for line in fin.readlines():\n",
    "        text = line.strip()\n",
    "        documents_list.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epO6fJPE1B0s"
   },
   "source": [
    "## **Membuat Fitur TF-IDF**\n",
    "Setelah berhasil meload dokumen langkah selanjutnya ialah mengenerate fitur TF-IDF pada dokumen. Pada proses ini juga dilakukan operasi prepocessing, yaitu case folding, stopword, dan tokenizing. Untuk melakukan proses ini dengan menggunakan RegexpTokenizer dari library nltk seperti source code berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1157,
     "status": "ok",
     "timestamp": 1666651050624,
     "user": {
      "displayName": "sumina so",
      "userId": "07390640577398086821"
     },
     "user_tz": -420
    },
    "id": "6hof42h0tz0S",
    "outputId": "b39b9fbb-c5ad-43f6-ba5f-6fab0e0acbf4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<12x236 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 412 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize regex tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Vectorize document using TF-IDF\n",
    "tfidf = TfidfVectorizer(lowercase=True,\n",
    "                        stop_words='english',\n",
    "                        ngram_range = (1,1),\n",
    "                        tokenizer = tokenizer.tokenize)\n",
    "\n",
    "# Fit and Transform the documents\n",
    "train_data = tfidf.fit_transform(documents_list)  \n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yw_YCNyu1O5_"
   },
   "source": [
    "## **Membuat Matrik SVD**\n",
    "Matrik SVD adalah teknik dekomposisi matriks yang memfaktorkan matriks dalam produk matriks. Untuk dapat membuat matrik tersebut kita dapat menggunakan TruncatedSVD dari library sklearn seperti berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1666651050625,
     "user": {
      "displayName": "sumina so",
      "userId": "07390640577398086821"
     },
     "user_tz": -420
    },
    "id": "QjAGqG9Ntnyv",
    "outputId": "5ff72ca1-6732-483a-810c-d45490ac8c1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.02366178e-02, -1.66382861e-02,  3.00062190e-02, ...,\n",
       "         1.35817657e-01, -1.98807570e-03,  7.74902033e-03],\n",
       "       [ 8.38985795e-02,  2.10631147e-01, -4.69070221e-02, ...,\n",
       "         1.82148827e-02, -2.80810446e-02, -1.39858160e-01],\n",
       "       [ 2.10849282e-02, -1.84815299e-02,  2.61033720e-03, ...,\n",
       "        -3.35817930e-02, -4.12961820e-04, -4.39328074e-03],\n",
       "       ...,\n",
       "       [ 2.52767786e-02, -2.00135859e-02, -5.03940302e-02, ...,\n",
       "        -3.07241369e-02,  1.88065252e-04, -2.37049787e-02],\n",
       "       [ 1.58110182e-02, -2.27024106e-02,  5.80867655e-02, ...,\n",
       "        -2.65079399e-02, -1.14091743e-02, -1.09058826e-02],\n",
       "       [ 2.27840084e-01, -1.53073584e-01, -1.05864584e-03, ...,\n",
       "        -4.25488190e-02, -4.07617741e-03,  7.17080829e-02]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "# Define the number of topics or components\n",
    "num_components=12\n",
    "\n",
    "# Create SVD object\n",
    "lsa = TruncatedSVD(n_components=num_components, n_iter=100, random_state=42)\n",
    "\n",
    "# Fit SVD model on data\n",
    "lsa.fit_transform(train_data)\n",
    "\n",
    "# Get Singular values and Components \n",
    "Sigma = lsa.singular_values_  \n",
    "V_transpose = lsa.components_.T\n",
    "V_transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PUxh1u21SxN"
   },
   "source": [
    "## **Ekstrak topik dan istilah**\n",
    "Setelah membuar matriks SVD, Selanjutnya kita perlu mengekstrak topik dari matriks komponen SVD dengan source code seperti berikut. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1666651050626,
     "user": {
      "displayName": "sumina so",
      "userId": "07390640577398086821"
     },
     "user_tz": -420
    },
    "id": "2IJSROA_uH7T",
    "outputId": "cdc418f0-a1e3-4b35-ffa1-eb1dd9fa30eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:twisted:/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: builtins.FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  ['negara', 'dan', 'yang', 'di', 'dalam']\n",
      "Topic 1:  ['elemen', 'ilmu', 'kunci', 'pengetahuan', 'adalah']\n",
      "Topic 2:  ['kemudian', 'juga', 'pendekatan', 'disebut', 'berbagai']\n",
      "Topic 3:  ['news', 'akan', 'sistem', 'pendekatan', 'kemudian']\n",
      "Topic 4:  ['bahan', 'korea', 'sangat', 'selatan', 'taiwan']\n",
      "Topic 5:  ['hal', 'baik', 'ini', 'cendekiawan', 'diskursus']\n",
      "Topic 6:  ['seringkali', 'banyak', 'sektor', 'cendekiawan', 'diskursus']\n",
      "Topic 7:  ['dan', 'juga', 'nies', 'antaranya', 'kelembagaan']\n",
      "Topic 8:  ['dokumen', 'bahan', 'korea', 'sangat', 'selatan']\n",
      "Topic 9:  ['ekonomi', 'bagian', 'begitu', 'berupaya', 'catch']\n",
      "Topic 10:  ['halnya', 'indonesia', 'langsung', 'menerapkan', 'mengabsorbsi']\n",
      "Topic 11:  ['amerika', 'austria', 'bagi', 'bahasa', 'belanda']\n"
     ]
    }
   ],
   "source": [
    "# Print the topics with their terms\n",
    "terms = tfidf.get_feature_names()\n",
    "\n",
    "for index, component in enumerate(lsa.components_):\n",
    "    zipped = zip(terms, component)\n",
    "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:5]\n",
    "    top_terms_list=list(dict(top_terms_key).keys())\n",
    "    print(\"Topic \"+str(index)+\": \",top_terms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_mDILoTHXYo"
   },
   "source": [
    "## **Kesimpulan**\n",
    "Hasil yang didapatkan dari topik modelling dengan Latent Semantic Indexing (LSI) atau Latent Semantic Analysis menggunakan library scikit-learn dengan mengambil 12 topik sebagai berikut.<br>\n",
    "Topic 1:  ['negara', 'dan', 'yang', 'di', 'dalam']<br>\n",
    "Topic 2:  ['elemen', 'ilmu', 'kunci', 'pengetahuan', 'adalah']<br>\n",
    "Topic 3:  ['kemudian', 'juga', 'pendekatan', 'disebut', 'berbagai']<br>\n",
    "Topic 4:  ['news', 'akan', 'sistem', 'pendekatan', 'kemudian']<br>\n",
    "Topic 5:  ['bahan', 'korea', 'sangat', 'selatan', 'taiwan']<br>\n",
    "Topic 6:  ['hal', 'baik', 'ini', 'cendekiawan', 'diskursus']<br>\n",
    "Topic 7:  ['seringkali', 'banyak', 'sektor', 'cendekiawan', 'diskursus']<br>\n",
    "Topic 8:  ['dan', 'juga', 'nies', 'antaranya', 'kelembagaan']<br>\n",
    "Topic 9:  ['dokumen', 'bahan', 'korea', 'sangat', 'selatan']<br>\n",
    "Topic 10:  ['ekonomi', 'bagian', 'begitu', 'berupaya', 'catch']<br>\n",
    "Topic 12:  ['halnya', 'indonesia', 'langsung', 'menerapkan', 'mengabsorbsi']<br>\n",
    "Topic 12:  ['amerika', 'austria', 'bagi', 'bahasa', 'belanda']"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPQTMO+dv7kWTMWUwhkP2UF",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}